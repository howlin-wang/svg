Motivated by the inherent limitations of **VAE-based latent diffusion models**, we propose **SVG (Self-supervised Visual Generation)**, a novel generative paradigm that constructs a **task-general feature space**. This space combines the **strong semantic discriminability** of modern vision foundation models with the **fine-grained perceptual details** necessary for high-quality image generation. The overall architecture of SVG is illustrated in Figure 3.

> (Here you would embed Figure 3 from the paper, showing the SVG Autoencoder architecture)

**Core Architecture: Fusing DINO and Residual Features**

The SVG autoencoder is meticulously designed to achieve two critical goals simultaneously: **preserve the rich semantic structure** of powerful self-supervised features and **ensure faithful image reconstruction**. It consists of two primary components:

1. **Frozen DINOv3 Encoder (Semantic Backbone)**
   - We leverage the exceptional capabilities of a pre-trained **DINOv3 encoder**.
   - DINOv3 is renowned for learning **highly discriminative features** that capture robust semantic information and strong inter-class separation.
   - By freezing this encoder, we directly incorporate its **semantically structured latent space**, which is crucial for efficient diffusion training.
   - Acts as the **semantic backbone**, providing a clean and organized foundation for generation.

2. **Lightweight Residual Encoder (Detail Refinement)**
   - DINOv3 may miss some **fine-grained perceptual details**.
   - Introduce a **lightweight Residual Encoder**, built on a **Vision Transformer**, to capture these missing details (color, texture, high-frequency patterns) **without interfering with DINOv3 semantic integrity**.

The outputs of the Residual Encoder are concatenated along the channel dimension with the DINOv3 features. This fusion process creates the comprehensive **SVG feature space**, enriched with both **semantic discriminability** and **essential perceptual details**, enabling **high-quality reconstruction**.

**Diffusion Modeling on a Unified Feature Space**

Unlike prior approaches that train diffusion models on low-dimensional VAE latent spaces, **SVG Diffusion** directly treats this **high-dimensional, semantically structured SVG feature space** as its target distribution. The diffusion model is trained using a **flow matching objective** (as defined in Equation 5 of the paper).

A key challenge with high-dimensional latent spaces in diffusion models is often **unstable convergence**. However, the inherently **well-dispersed semantic structure** of our SVG features—where different semantic classes are clearly separated and intra-class representations are compact (as shown in Figure 4)—makes training remarkably **stable and efficient**. This structured latent space simplifies optimization, allowing the diffusion model to **converge faster** and achieve **superior generative quality** with fewer sampling steps.

> By integrating DINOv3 and a residual branch, **SVG creates a unified feature representation** that not only excels in generative tasks but also inherently supports a **wide range of core vision tasks**, providing a versatile foundation for visual understanding.