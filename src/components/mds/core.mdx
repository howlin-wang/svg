We introduce **SVG** (**S**elf-supervised representation for **V**isual **G**eneration), a new paradigm for latent diffusion models (LDMs) that **completely eliminates the traditional Variational Autoencoder (VAE)**.

We raise and explore the following **key questions**:

> Given that generation and perception/understanding are inherently intertwined, **why not unify them within a shared feature space?**

> Can existing advanced visual foundation model (VFM) features—such as those from DINO—serve as an **effective representation for diffusion model training, with minimal modification?**

Our findings provide a clear answer: **such a unified feature space is entirely feasible.** The DINO feature space requires only minor adjustments to simultaneously **support high-quality generation and reconstruction.**

Our proposed SVG model leads to **transformative advantages**:

1. **Extreme Efficiency**: Compared to vanilla VAE-based models, SVG achieves up to **62x speedup in training** and a **35x speedup in inference**.
2. **Superior Quality**: Generates **high-fidelity and high-quality images** in few sampling steps.
3. **True Versatility**: Establishes a **unified visual feature space**. The same encoder used for **image generation** can, without any modification, achieve strong performance on downstream tasks like **image classification, semantic segmentation, and depth estimation**, realizing a *"one space, multiple purposes"* vision.

<figure style={{ textAlign: 'center' }}>
  <img src="./teaser.png" alt="teaser" height="410" style={{ border: '1px solid #ccc' }} />
  <figcaption>Fig 1: Core Contribution</figcaption>
</figure>

<figure style={{
  display: 'flex',
  flexDirection: 'column',
  alignItems: 'center'
}}>
  <img src="./viz.png" alt="viz" height="280" style={{ border: '1px solid #ccc' }} />
  <figcaption>Fig 2: Selected 256x256 samples from SVG-XL</figcaption>
</figure>